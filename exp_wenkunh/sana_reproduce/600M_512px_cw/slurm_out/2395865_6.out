I0415 00:39:39.156000 679785 site-packages/torch/distributed/run.py:675] Using nproc_per_node=8.
I0415 00:39:39.156000 3204684 site-packages/torch/distributed/run.py:675] Using nproc_per_node=8.
W0415 00:39:39.156000 3204684 site-packages/torch/distributed/run.py:792] 
W0415 00:39:39.156000 3204684 site-packages/torch/distributed/run.py:792] *****************************************
W0415 00:39:39.156000 3204684 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0415 00:39:39.156000 3204684 site-packages/torch/distributed/run.py:792] *****************************************
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194] Starting elastic_operator with launch configs:
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194]   entrypoint       : train_scripts.train
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194]   min_nodes        : 2
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194]   max_nodes        : 2
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194]   nproc_per_node   : 8
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194]   run_id           : 28439
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194]   rdzv_backend     : c10d
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194]   rdzv_endpoint    : 10.65.30.15:29500
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194]   rdzv_configs     : {'timeout': 900}
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194]   max_restarts     : 0
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194]   monitor_interval : 0.1
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194]   log_dir          : /tmp/torchelastic_fpnagewj
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194]   metrics_cfg      : {}
I0415 00:39:39.157000 3204684 site-packages/torch/distributed/launcher/api.py:194] 
I0415 00:39:39.168000 3204684 site-packages/torch/distributed/elastic/agent/server/api.py:860] [default] starting workers for entrypoint: python
I0415 00:39:39.169000 3204684 site-packages/torch/distributed/elastic/agent/server/api.py:677] [default] Rendezvous'ing worker group
W0415 00:39:39.156000 679785 site-packages/torch/distributed/run.py:792] 
W0415 00:39:39.156000 679785 site-packages/torch/distributed/run.py:792] *****************************************
W0415 00:39:39.156000 679785 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0415 00:39:39.156000 679785 site-packages/torch/distributed/run.py:792] *****************************************
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194] Starting elastic_operator with launch configs:
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194]   entrypoint       : train_scripts.train
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194]   min_nodes        : 2
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194]   max_nodes        : 2
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194]   nproc_per_node   : 8
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194]   run_id           : 28439
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194]   rdzv_backend     : c10d
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194]   rdzv_endpoint    : 10.65.30.15:29500
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194]   rdzv_configs     : {'timeout': 900}
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194]   max_restarts     : 0
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194]   monitor_interval : 0.1
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194]   log_dir          : /tmp/torchelastic_jcsy_f8i
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194]   metrics_cfg      : {}
I0415 00:39:39.157000 679785 site-packages/torch/distributed/launcher/api.py:194] 
I0415 00:39:39.170000 679785 site-packages/torch/distributed/elastic/agent/server/api.py:860] [default] starting workers for entrypoint: python
I0415 00:39:39.171000 679785 site-packages/torch/distributed/elastic/agent/server/api.py:677] [default] Rendezvous'ing worker group
srun: Job step aborted: Waiting up to 122 seconds for job step to finish.
Apr 15 00:39:40.107601 679659 slurmstepd   0x15555286c640: error: *** JOB 2395943 ON cw-dfw-h100-004-337-012 CANCELLED AT 2025-04-15T00:39:40 ***
Apr 15 00:39:40.107817 679774 slurmstepd   0x15555191e640: error: *** STEP 2395943.1 ON cw-dfw-h100-004-337-012 CANCELLED AT 2025-04-15T00:39:40 ***
